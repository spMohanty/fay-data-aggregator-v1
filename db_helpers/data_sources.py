"""Food Project data wrangling

Time conventions:
* "loc_" prefix for local times and dates. Otherwise, assume UTC.
* "_at" suffix for timestamps
* "_on" suffix for dates
* "dow" means day-of-week
* tz-unaware times in the DB should be read as tz-aware UTC times
"""
import sys
from pathlib import Path

import numpy as np
import pandas as pd

# pylint: disable=import-error,wrong-import-position
CUR_DIR = Path(__file__).resolve().parent
sys.path.append(str(CUR_DIR))
import database as db

# pylint: enable=import-error,wrong-import-position

SQL_DIR = CUR_DIR / "db" / "sql"
CACHE_DIR = CUR_DIR / "cache"


class Data:
    """Consistent way to access data from the various food projects"""

    caches = {}

    # get mfr data
    @classmethod
    def get_dishes(cls):
        """Get MFR dish data.

        First time caches. Subsequent calls return cached copy.

        Returns
        -------
        pd.DataFrame
        """
        key = "dish"
        if key not in cls.caches:
            cls.caches[key] = cls._fetch_dishes_all()
        return cls.caches[key].copy()

    @classmethod
    def _fetch_dishes_all(cls):
        timestamp_cols = ["eaten_at", "local_eaten_at"]
        dtypes = {col: "object" for col in timestamp_cols}
        dtypes["barcode"] = "string"
        dtypes["standard_portion_unit"] = "string"
        print(CACHE_DIR, "mfr_data_v1.csv")
        # get raw data generated by R
        df = pd.read_csv(CACHE_DIR / "mfr_data_v1.csv", dtype=dtypes, low_memory=True)
        # filter to only tracked participants
        fpath = SQL_DIR / "fanal_participants_tracked.sql"
        with open(fpath, "r", encoding="utf-8") as file:
            df = df.merge(db.AnalyticsDb().select(file.read()), on="subject_key")
        # format timestamp columns
        for col in timestamp_cols:
            # df[col] = pd.to_datetime(df[col], utc=True, format="%Y-%m-%d %H:%M:%S UTC")
            df[col] = pd.to_datetime(df[col],  utc=True, format="mixed")
        # add hour column
        df["loc_eaten_hour"] = (
            df["local_eaten_at"].dt.hour
            + df["local_eaten_at"].dt.minute / 60
            + df["local_eaten_at"].dt.second / 3600
        )
        # add date column
        df["loc_eaten_on"] = df["local_eaten_at"].dt.date.astype("datetime64[ns]")
        # add day-of-week column
        df["loc_eaten_dow"] = df["loc_eaten_on"].dt.day_name().str[:3]
        df["loc_eaten_dow_type"] = np.where(
            df["loc_eaten_on"].dt.dayofweek < 5, "weekday", "weekend"
        )
        # add season column (meteorological, not astronomical)
        season_months = np.array(range(12)) + 1
        season_names = ["Winter"] * 3 + ["Spring"] * 3 + ["Summer"] * 3 + ["Fall"] * 3
        season_map = dict(zip(season_months, np.roll(season_names, -1)))
        df["loc_eaten_season"] = df["loc_eaten_on"].dt.month.map(season_map)
        # calculate missing kcal from kJ
        df["energy_kcal_eaten"] = df["energy_kcal_eaten"].fillna(
            df["energy_kj_eaten"] / 4.184
        )
        # remove unused columns
        df.drop(
            columns=[
                df.columns[0],  # "Unnamed: 0"
                "local_eaten_at",
                "eaten_at_utc_offset",
                "media_count",
                "display_name_fr",
                "display_name_de",
                "standard_portion_quantity",
                "standard_portion_unit",
                "specific_gravity",
                "alcohol",
                "all_trans_retinol_equivalents_activity",
                "beta_carotene",
                "beta_carotene_activity",
                "calcium",
                "carbohydrates",
                "chloride",
                "cholesterol",
                "energy_kcal",
                "energy_kj",
                "fat",
                "fatty_acids_monounsaturated",
                "fatty_acids_polyunsaturated",
                "fatty_acids_saturated",
                "fiber",
                "folate",
                "iodide",
                "iron",
                "magnesium",
                "niacin",
                "pantothenic_acid",
                "phosphorus",
                "potassium",
                "protein",
                "salt",
                "sodium",
                "starch",
                "sugar",
                "vitamin_a_activity",
                "vitamin_b1",
                "vitamin_b12",
                "vitamin_b2",
                "vitamin_b6",
                "vitamin_c",
                "vitamin_d",
                "vitamin_e_activity",
                "water",
                "zinc",
                "energy_kj_eaten",
            ],
            inplace=True,
        )
        df.sort_values(
            by=["fay_user_id", "eaten_at", "dish_id", "food_id"], inplace=True
        )
        return df

    # get glucose query
    @classmethod
    def get_glucose_query(cls):
        """Get SQL query for glucose data."""
        fpath = SQL_DIR / "fanal_glucose_filtered.sql"
        with open(fpath, "r", encoding="utf-8") as file:
            return db.ParameterizedQuery(file.read())

    # get glucose data
    @classmethod
    def get_glucose(cls):
        """Get glucose data."""
        key = "glucose"
        if key not in cls.caches:
            cls.caches[key] = cls._fetch_glucose_df()
        return cls.caches[key].copy()

    @classmethod
    def _fetch_glucose_df(cls):
        df = db.AnalyticsDb().select(cls.get_glucose_query())
        df["loc_read_hour"] = (
            df["loc_read_at"].dt.hour
            + df["loc_read_at"].dt.minute / 60
            + df["loc_read_at"].dt.second / 3600
        )
        # add date column
        df["loc_read_on"] = df["loc_read_at"].dt.date.astype("datetime64[ns]")
        # add day-of-week column
        df["loc_read_dow"] = df["loc_read_on"].dt.day_name().str[:3]
        df["loc_read_dow_type"] = np.where(
            df["loc_read_on"].dt.dayofweek < 5, "weekday", "weekend"
        )
        df.drop(columns=["loc_read_at"], inplace=True)
        return df

    # get sleep data
    # Subjective data from morning surveys.
    # Objective data from analytics DB, parsed from health files
    @classmethod
    def get_sleep(
        cls, min_duration=4, max_duration=12, gap_merge=pd.Timedelta(minutes=30)
    ):
        """Get sleep data."""
        key = f"sleep|{min_duration}|{max_duration}|{gap_merge}"
        if key not in cls.caches:
            cls.caches[key] = cls._fetch_sleep_all(
                min_duration, max_duration, gap_merge
            )
        return cls.caches[key].copy()

    @classmethod
    def _fetch_sleep_all(cls, min_duration, max_duration, gap_merge):
        # subjective
        db_users = db.AnalyticsDb().select(
            """
            SELECT
                p.id AS user_id,
                p.cohort,
                p.timezone AS tz
            FROM faydesc_participants p
        """
        )
        fpath = SQL_DIR / "fay_sleep_all.sql"
        with open(fpath, "r", encoding="utf-8") as file:
            df = db.FayDb().select(file.read())
        df = df.merge(db_users, on="user_id")
        for moment in ["started", "ended"]:

            def _inner(row, moment=moment):
                val_raw = row[f"raw_{moment}_at"]
                val_naive = val_raw.tz_localize(None)
                val_local = val_naive.tz_localize(row["tz"])
                return val_local.tz_convert("UTC")

            df[f"{moment}_at"] = df.apply(_inner, axis=1)
        df["duration_hours"] = (
            df["ended_at"] - df["started_at"]
        ).dt.total_seconds() / 3600
        df = df[
            df["duration_hours"].between(min_duration, max_duration, inclusive="left")
        ]
        df.drop(["raw_started_at", "raw_ended_at", "tz"], axis=1, inplace=True)
        df["src"] = "subjective"
        subjective = df

        # objective (but prefer subjective for dupes)
        fpath = SQL_DIR / "fanal_sleep_all.sql"
        with open(fpath, "r", encoding="utf-8") as file:
            df = db.AnalyticsDb().select(file.read())
        df = df[~df["user_id"].isin(subjective["user_id"])]
        df.sort_values(by=["user_id", "started_at"], inplace=True)
        ranges = []
        for _, row in df.iterrows():
            if not ranges or (
                row["user_id"] != ranges[-1]["user_id"]
                or row["started_at"] > (ranges[-1]["ended_at"] + gap_merge)
            ):
                ranges.append(row.to_dict())
            else:
                ranges[-1]["ended_at"] = max(ranges[-1]["ended_at"], row["ended_at"])
        df = pd.DataFrame(ranges)
        df = df.sort_values(
            by=["user_id", "loc_occurred_on", "duration_hours"],
            ascending=[True, True, False],
        )
        df = df.drop_duplicates(subset=["user_id", "loc_occurred_on"])
        df = df[
            df["duration_hours"].between(min_duration, max_duration, inclusive="left")
        ]
        df["src"] = "objective"
        objective = df

        # combine and finalize
        df = pd.concat([subjective, objective])
        df["loc_dow"] = df["loc_occurred_on"].dt.day_name().str[:3]
        df["loc_dow_type"] = np.where(
            df["loc_occurred_on"].dt.dayofweek < 5, "weekday", "weekend"
        )
        return df


class Helper:
    """Helper functions for data wrangling"""

    @classmethod
    def human_readable_number(cls, num, fmt=None):
        """Format a number"""
        if fmt is None:
            if type(num) in [int, np.int64]:
                fmt = ",d"
            elif type(num) in [float, np.float64]:
                fmt = ",.2f"
            else:
                raise ValueError("Unhandled number type, please specify format")
        return format(num, fmt).replace(",", "'")

    @classmethod
    def get_stats_circular(cls, series, mod):
        """Calculate circular mean and standard deviation of a series

        Args:
            series (pd.Series): Series of values on a circle
            mod (float): Modulus of circle (e.g. 24 for hours)
        Returns:
            dict: `mean` and `std` keys

        The results are found through trigonometry.
        `mean` is the arctan of the means of sin and cos
        `std` is uses the formula from "Mardia (1972)"
            with r as mean length
                `std = sqrt(-2 ln r)`
            simplified to
                `std = sqrt(-ln r**2)`
        """
        circle = 2 * np.pi
        conversion = circle / mod
        angles = series * conversion

        # perform math on sin and cos
        mean_sin = np.mean(np.sin(angles))
        mean_cos = np.mean(np.cos(angles))
        mean_angle = np.mod(np.arctan2(mean_sin, mean_cos) + circle, circle)
        mean = mean_angle / conversion

        squared_distance = mean_sin**2 + mean_cos**2
        if squared_distance >= 1.0:
            # Avoid sqrt(negative). Usually due to `len(series.nunique()) == 1`
            std = 0
        else:
            std_angle = np.sqrt(-np.log(squared_distance))
            std = std_angle / conversion

        return {"mean": mean, "std": std}
